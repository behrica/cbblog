---
title:  Querying Zenodo.org repository with R
date:  2016-10-18T21:13:14-05:00
published: true
tags: ['OAI-PMH', 'zenodo', 'R'] 

liftr:
  from: rocker/hadleyverse
  pandoc: false
  cranpkg:
    - oai
  maintainer: Carsten Behring
  maintainer_email: carsten.behring@gmail.com
---

# Zenodo 
[Zenodo](http://zenodo.org) is a repository which allows everybody to deposit free of charge any type of research output, in all disciplines of science.

EFSA is piloting it's use for creating a knowledge base on all types of food safety related evidence(data, documents, models).

Zenodo has an API and can be queried using the standard OAI-PMH protocol, which allows to harvest the metadata and all deposits.

# 'oai' package 

R has a package available to query any OAI-PMH repository, including Zenodo.
It can be installed from CRAN like this:

```{r eval=FALSE}
install.packages("oai")
```

The development version is available on Github at 
<https://github.com/ropensci/oai>


The libraries I use in this tutorial are: 
```{r echo=TRUE,message=FALSE,warning=FALSE}
library(knitr)
library(tidyverse)
library(httr)
library(oai)
library(xml2)
opts_chunk$set(echo=T)

```

# Retreive records from Zenodo

The oai package  allows to retrieve all records of a given Zenodo community, in this case the EFSA pilot community.
The following code shows all records of a community with their digital object identifier and the title. 

```{r cache=TRUE}
record_list<- list_records("https://zenodo.org/oai2d",metadataPrefix="oai_datacite",set="user-efsa-pilot")

kable(record_list %>% select(identifier.3,title))

```

Currently there are `r nrow(record_list)` records available.

## Statistics on keywords

### Query records from Zenodo

I was further on interested in the current distribution of keywords each record was tagged with. Zenodo supports two types of keywords. Simple free text keywords and 'subjects'.
Subjects need to come from  a controlled vocabulary, in which each topic has an URI.

EFSA uses the [GACS](http://browser.agrisemantics.org/gacs/en/) vocabulary, and so a certain topic 'salmonella' is represented as URI 'http://browser.agrisemantics.org/gacs/en/page/C2225'.

The API returns therefore for the subjects only the URI, which is nicely unique and clear but not user friendly as a label. On the URI of each 'subject', additional information is available.



The following code retrieves all records and extract all their subjects (which have a Xpath of //d3:subject). The current oai package has some problems with some Zenodo specific metadata,
so I parse the raw XML by hand.

The OIA-PMH standard and the oai::get_records function, allow the client to select, in which metadata format he wants to receive the metadata.
Here I have selected 'oai-datacite', because it is recommended from the 
Zenodo API [documenation](https://zenodo.org/dev#harvest-metadata) and should contain *all* metadata Zenodo supports, while other metadata formats might only support a smaller subset.

```{r cache=TRUE}


record_data_xml <- get_records(record_list$identifier,url="https://zenodo.org/oai2d",prefix="oai_datacite",as="raw")  
keyword_counts <- record_data_xml %>%
    map(read_xml) %>%
    map(xml_find_all,"//d3:subject") %>%
    map(xml_text) %>%
    reduce(c) %>%
    table() %>%
    tbl_df()
kable(keyword_counts %>% filter(grepl(".*C22.*|^food",`.`)))
```

I use the 'map' function from the 'purrr' package to apply to every vector in the result (which is first an xml string) a number of transformations:

1.  read_xml() - to convert from string to class xml_document
2.  xml_find_all() - to find all xml nodes given by xpath expression
3.  xml_text() - get the text from the xml node
 
 Then I combine all this via c() and the reduce() function to obtain a single list of all subjects.
 
 The API returns both types of subjects, the generic keywords and the terms referring to a controlled vocabulary.
 
 The table() command produces then a frequency table for them, of which I show here a subset.
 We have in this table entries with an English label, and some with the GACS URI.
 
### Add human readable label to GACS topics


To add a human readable label to each GACS URI, I use the GACS API which allows to query information on each topic.
So I call the API for each URI and make a table where each row contains a list of (URI,label). This gets the converted into a table with bind_rows()

I use again the 'map' function with an anonymous function, which does the call to the GACS API. GACS uses the (Skomsos)[https://github.com/NatLibFi/Skosmos] software, so has an (API)[http://api.finto.fi/doc/] to query the vocabulary.
                                        
```{r cache=TRUE}
gacs <- keyword_counts %>% filter(grepl("*gacs*",.))

gacs_label_en <- map(gacs$`.`,function(uri) {


    r=GET("http://browser.agrisemantics.org/rest/v1/gacs/label",query=list(uri=uri,lang="en"))
    list(uri=uri,label=content(r)$prefLabel)
    
}) %>%
    bind_rows()
kable(gacs_label_en[1:5,])
```



## Distributions of labels in efsa-pilot community

To get the final table, I join the label-GACS pairs with the former table and do some clean-up with the 
functions from tidyr package.

The table is then sorted by frequency and shown on the screen.

As we can see, the most frequent words are 'risk assessment' and 'exposure assessment', which is no surprise as these is the core of EFSA's
scientific work.

```{r}

table <- left_join(keyword_counts,gacs_label_en,by=c("."="uri")) %>% 
  replace_na(list(label="")) %>%
  unite("label",c(label,`.`),sep=" - ") %>%
  mutate(label = gsub("^ - ","",label)) %>%
  rename(count=n) %>%
  arrange(-count)

write.csv(table,"keywords.csv",row.names = F)
knitr::kable(table %>% slice(1:20))

    
```

To monitor regularly this distribution can help in keeping the list of all keywords clean and eventually propose additional subjects to the GACS vocabulary.


# Session info
```{r}
sessionInfo()
```


